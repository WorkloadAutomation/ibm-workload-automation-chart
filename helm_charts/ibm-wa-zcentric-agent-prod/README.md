
# IBM Workload Automation Z-Centric Agent on Kubernetes 

## Introduction
To ensure a fast and responsive experience when using IBM Workload Automation, you can deploy IBM Workload Automation on a cloud infrastructure. A cloud deployment ensures access anytime anywhere and is a fast and efficient way to get up and running quickly. It also simplifies maintenance, lowers costs, provides rapid upscale and downscale, minimizes IT requirements and physical on-premises data storage.

As more and more organizations move their critical workloads to the cloud, there is an increasing demand for solutions and services that help them easily migrate and manage their cloud environment.

To respond to the growing request to make automation opportunities more accessible, IBM Workload Automation containers can be deployed into the following supported third-party cloud provider infrastructures:

- ![Amazon EKS](images/tagawseks.png "Amazon EKS") Amazon Web Services (AWS) Elastic Kubernetes Service (EKS)
- ![Microsoft Azure](images/tagmsa.png "Microsoft Azure") Microsoft&reg; Azure Kubernetes Service (AKS)
- ![Google GKE](images/taggke.png "Google GKE") Google Kubernetes Engine (GKE)

IBM Workload Automation is a complete, modern solution for batch and real-time workload management. It enables organizations to gain complete visibility and control over attended or unattended workloads. From a single point of control, it supports multiple platforms and provides advanced integration with enterprise applications including ERP, Business Analytics, File Transfer, Big Data, and Cloud applications.

The information in this README contains the steps for deploying the following IBM workload Automation components using an Helm chart and contaner images: 
- z-Centric Agent 

For more information about IBM Workload Automation, see the product documentation library in [IBM Documentation]( https://www.ibm.com/docs/workload-scheduler/10.1.0).
 
### Helm Chart details

IBM Workload Automation can be deployed across a single cluster, but you can add multiple instances of the product components by using a different namespace in the cluster. The product components can run in multiple failure zones in a single cluster.

By default, this Helm chart installs a single z-Centric agent. In addition to the product components, the following Kubernetes objects are installed: 
 
  
|  |Agent  |
|--|--|
| **Deployments** |   |
|**Pods**  |wa-waagent-0  |
|**Stateful Sets**  |wa-waagent for dynamic agent |
|**Secrets**  | |
|**Certificates (Secret)**  |wa-waagent |
|**Network Policy**  |da-network-policy | 
|**Services**  |wa-waagent-h  |
|**PVC** (generated from Helm chart). Default deployment includes a single (replicaCount=1) server, console, agent. Create a PVC for each instance of each component.  | 1 PVC data-wa-waagent-waagent0 |
|**PV** (Generated by PVC)  | 1 PV |
|**Service Accounts**  |  |
|**Roles**  |wa-pod-role  | 
|**Role Bindings**  |wa-pod-role-binding |
|**Cluster Roles**  | {{ .Release.Namespace }}-wa-pod-cluster-role-get-routes (name of clusterRole and, where {{ .Release.Namespace }} represents the name of the namespace \<workload_automation_namespace>) |
|**Cluster Role Bindings**  |{{ .Release.Namespace }}-wa-pod-cluster-role-get-routes-binding (name of ClusterRoleBinding and, where {{ .Release.Namespace }} represents the name of the namespace \\<workload_automation_namespace>) |
|**Ingress** or **Load Balancer**|  Depends on the type of network enablement that is configured. See [Network enablement](#network-enablement)  |

 **Data encryption**: 
 * Data in transit encrypted using TLS V1.2
 * Data at rest encrypted using passive disk encryption. 
 * Secrets are stored in Kubernetes Secrets.
 * Logs are clear of all sensitive information. 

### Supported Platforms

- ![Amazon EKS](images/tagawseks.png "Amazon EKS") Amazon Elastic Kubernetes Service (EKS) on amd64: 64-bit Intel/AMD x86
- ![Microsoft Azure](images/tagmsa.png "Microsoft Azure") Azure Kubernetes Service (AKS) on amd64: 64-bit Intel/AMD x86  
- ![Google GKE](images/taggke.png "Google GKE") Google Kubernetes Engine (GKE) on amd64: 64-bit Intel/AMD x86
- RedHat OpenShift on amd64: 64-bit Intel/AMD x86
	
###  Accessing the container images

You can access the IBM Workload Automation chart and container images from the Entitled Registry. See [Create the Entitlement Secret](#create-the-secret) for more information about accessing the registry. The images are as follows:

* cp.icr.io/cp/ibm-workload-automation-agent-dynamic:10.1.0.01.20221130
 
## Prerequisites

Before you begin the deployment process, ensure that your environment meets the following prerequisites:

#### Cloud environment
- **Kubernetes** installed and running. Version >=1.15 (no specific APIs need to be enabled).
- **cert-manager**. Jestack's [cert-manager](https://cert-manager.io) adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates. 

(Optional) The following are prerequisites specific to each supported cloud provider:

![Amazon EKS](images/tagawseks.png "Amazon EKS") 
- Amazon Kubernetes Service (EKS) installed and running
- AWS CLI (AWS command line)

![Microsoft Azure](images/tagmsa.png "Microsoft Azure") 
- Azure Kubernetes Service (AKS) installed and running
- azcli (Azure command line)

![Google GKE](images/taggke.png "Google GKE") 
- Google Kubernetes Engine (GKE) installed and running
- gcloud SDK (Google command line)

#### Local environment

- An API Entitlement key for accessing IBM Entitled Registry: `cp.icr.io`
- **Helm** Version >= 3.0. [Helm](https://helm.sh/) is the package manager for Kubernetes,
- **kubectl**   The Kubernetes command-line tool, [kubectl](https://kubernetes.io/docs/reference/kubectl/kubectl/), allows you to run commands against Kubernetes clusters.
- (Optional) **OpenSSL** to generate custom certificates. _OpenSSL_ is a robust, commercial-grade, full-featured Open Source toolkit to manage TLS.

## Resources Required
  
 The following resources correspond to the default values required to manage a production environment. These numbers might vary depending on the environment.
 
| Component | Container resource limit | Container memory request |
|--|--|--|
| **z-Centric Agent** |CPU: 1, Memory: 2Gi |CPU: 200m, Memory: 200Mi, Storage size: 2Gi  |

#### Storage classes static PV and dynamic provisioning

![Amazon EKS](images/tagawseks.png "Amazon EKS") 
| Provider            | Disk Type    | PVC Size | PVC Access Mode |
| ------------------- | ------------ | -------- |---------------- |
| AWS EBS             | GP2 SSD      | Default  | ReadWriteOnce   |
| AWS EBS             | IO1 SSD      | Default  | ReadWriteOnce   |

For additional details about AWS storage settings, see [Storage classes](https://docs.aws.amazon.com/eks/latest/userguide/storage-classes.html).

![Microsoft Azure](images/tagmsa.png "Microsoft Azure") 
| Provider               | Disk Type    | PVC Size | PVC Access Mode |
| ---------------------- | ------------ | -------- |---------------- |
| Azure File             |     SSD      | Default  | ReadWriteOnce   |
| Azure Disk             |     SSD      | Default  | ReadWriteOnce   |

**Note:** The volumeBindingMode must be set to **WaitforFirstConsumer** and not **Immediate**. 
For additional details about Microsoft Azure storage settings, see [Azure Files - Dynamic](https://docs.microsoft.com/en-us/azure/aks/azure-files-dynamic-pv).

![Google GKE](images/taggke.png "Google GKE")
| Provider               | Disk Type                | PVC Size | PVC Access Mode |
| ---------------------- | -------------------------| -------- |---------------- |
| GCP                    |Standard Persistent Disks | Default  | ReadWriteOnce   |
| GCP                    |Balanced Persistent Disks | Default  | ReadWriteOnce   |
| GCP                    |SSD Persistent Disks      | Default  | ReadWriteOnce   |


For more details about the storage requirements for your persistent volume claims, see the **[Storage](#storage)** section of this README file.

## Installing
Installing and configuring the IBM Workload Automation components, involves the following high-level steps:

1. [Creating the Namespace](#create-the-namespace).
2. [Creating an Entitlement Secret](#create-the-secret)  to store on your cluster an entitlement key for the IBM Workload Automation offering.
3. [Securing communication](#securing-communication) using your custom certificates. Create a secret to store password and certificates.
5. [Deploying the product components](#deploying-the-product-components).
6. [Verifying the installation](#verifying-the-installation).

#### Additional integration steps
1. [Installing Automation Hub integrations](#installing-automation-hub-integrations).
2. [Installing custom integrations](#installing-custom-integrations).


### 1. Creating the Namespace

To create the namespace, run the following command:

        kubectl create namespace <workload_automation_namespace>
	

### 2. Creating an Entitlement Secret 

Obtain your entitlement key and store it on your cluster by creating a [Kubernetes Secret](https://kubernetes.io/docs/concepts/configuration/secret/). Using a Kubernetes secret allows you to securely store the key on your cluster and access the registry to download the chart and product images. 

1. Access the entitled registry. 
Log in to [My IBM Container Software Library](https://myibm.ibm.com/products-services/containerlibrary) with the id and password that are associated with the entitled software.


2.  In the **Container software library** tile, click **View library** then click **Copy key** to copy the entitlement key to the clipboard. 


3. To create a pull secret for your entitlement key that enables access to the entitled registry, run the following command:

         kubectl create secret docker-registry -n <workload_automation_namespace> sa-<workload_automation_namespace> --docker-server=<registry_server> --docker-username=<user_name> --docker-password=<password>
	   
	where,
	* <workload_automation_namespace> represents the namespace where the product components are installed
	* <registry_server> is `cp.icr.io`
	* <user_name> is `cp`
	* \<password> is the entitled key copied from the entitled registry `<api_key>`

		
### 3. Securing communication 

You secure communication using certificates. You can manage certificates using your own custom certificates. For information about using your own certificates, see the section [Configuring](#configuring). 
	
1. Create the Certificate Authority (CA) by running the following commands:

   a.   `.\openssl.exe genrsa -out ca.key 2048`
    
   b.   `.\openssl.exe req -x509 -new -nodes -key ca.key -subj "/CN=WA_ROOT_CA" -days 3650 -out ca.crt`
	
2.  Create the CA key pair secret by running the following command:

        kubectl create secret tls ca-key-pair --cert=ca.crt --key=ca.key -n <workload_automation_namespace>

3. Create the Issuer under the namespace. Edit the issuer.yaml file with the namespace and CA key pair.

    a. Create the issuer.yaml as follows, specifying the namespace and CA key pair:
	
        apiVersion: cert-manager.io/v1alpha2
        kind: Issuer
        metadata:
          labels:
            app.kubernetes.io/name: cert-manager
          name: wa-ca-issuer
          namespace: <workload_automation_namespace>
        spec:
          ca:
            secretName: ca-key-pair

    b. Run the following command to create the issuer under the namespace:		  

        kubectl apply -f issuer.yaml -n <workload_automation_namespace>
		
#### Creating a secrets file
Create a secrets file to store passwords for the agents or if you use custom certificates, to add your custom certificates to the Certificates Secret.

##### Creating secrets file to store passwords for the agents

1. Manually create a mysecret.yaml file to store passwords. The mysecret.yaml file must contain the following parameters:

	    apiVersion: v1
	    kind: Secret
	    metadata:
	      name: wa-pwd-secret
	      namespace: <workload_automation_namespace>
	    type: Opaque
	    data:
	       WA_PASSWORD: <hidden_password>
	       
where:
     
   - **wa-pwd-secret** is the value of the pwdSecretName parameter defined in the [Configuration Parameters](#configuration-parameters) section;    
   - **<workload_automation_namespace>** is the namespace where you are going to deploy the IBM Workload Automation product components.  
   - **<hidden_password>** must be entered; to enter an encrypted password, run the following command in a UNIX shell and copy the output into the yaml file:
	    `echo -n 'mypassword' | base64`

> **Note**: The `echo` command must be launched separately for each password that you want to enter as an encrypted password in the mysecret.yaml:
   - WA_PASSWORD: \<hidden password>
     

2. After the file has been created and filled in, it must be imported.

   a. From the command line, log in to the cluster.
	
   b. Apply the WA-Secret (wa-pwd-secret). Create a secrets file to store passwords for both the agents. Launch the following command:
	
	    kubectl apply -f <my_path>/mysecret.yaml -n <workload_automation_namespace>
	  
where **<my_path>** is the location path of the mysecret.yaml file.	


### 4. Deploying the product components		

To deploy the IBM Workload Automation components, ensure you have first downloaded the chart from the IBM Entitled Registry: `cp.icr.io` and have unpacked it to a local directory. If you already have the chart then update it.


1. Download the chart from the repository and unpack it to a local directory or, if you already have the chart, update it.

   **First time installation and configuration of the chart:**

   a. Add the repository:
   
        helm repo add <repo_name> https://workloadautomation.github.io/ibm-workload-automation-chart/stable, where <repo_name> represents the name of the chosen local repository
   
   b.  Update the Helm chart:
   
        helm repo update 
   
   c. Pull the Helm chart:

        helm pull <repo_name>/workload-automation-prod
      
     
**Update your chart:**

        helm repo update 	 
		
1. Customize the deployment. Configure each product component by adjusting the values in the `values.yaml` file. See these parameters and their default values in [Configuration Parameters](#configuration-parameters). By default, a z-centric or dynamic agent is installed.
	 
2. Deploy the instance by running the following command: 


        helm install -f values.yaml <workload_automation_release_name> <repo_name>/workload-automation-prod -n <workload_automation_namespace>


where, <workload_automation_release_name> is the deployment name of the instance. 

The following are some useful Helm commands:

* To list all of the Repo releases: 

        helm list -A
	
* To update the Helm release:

        helm upgrade <workload_automation_release_name> <repo_name>/ibm-workload-automation-prod -f values.yaml -n <workload_automation_namespace>
	
* To delete the Helm release: 

        helm uninstall <workload_automation_release_name> -n <workload_automation_namespace>
		

### Verifying the installation

After the deployment procedure is complete, you can validate the deployment to ensure that everything is working. 

To manually verify that the installation was successfully installed, you can perform the following checks:
 
1. Run the following command to verify the pods installed in the <workload_automation_namespace>:
   
	kubectl get pods -n <workload_automation_namespace>
	   
2. Locate the Agent pod name that is in the format `<workload_automation_release_name>-waagent-0`.   
		   
3. To access the Agent pod, open a bash shell and run the following command:
	
        kubectl exec -ti <workload_automation_release_name>-waagent-0 -n <workload_automation_namespace> -- /bin/bash
		kubectl logs <wa-agent-pod-name> -n <workload_automation_namespace>
		

  
## Additional integrations

### Installing Automation Hub integrations  

You can extend Workload Automation with a number of out-of-the-box integrations, or plug-ins. Complete documentation for the integrations is available on [Automation Hub](https://www.yourautomationhub.io/). Use this procedure to integrate only the integrations you need to automate your business workflows.

**Note:** You must perform this procedure before deploying the agents. Any changes made post-installation are applied the next time you perform an upgrade.

The following procedure describes how you can create and customize a *configMap* file to identify the integrations you want to make available in your Workload Automation environment:

1) Create a .yaml file, for example, **plugins-config.yaml**, with the following content. This file name will need to be specified in a subsequent step.

		####################################################################
		# Licensed Materials Property of HCL*
		# (c) Copyright HCL Technologies Ltd. 2022. All rights reserved.
		#
		# * Trademark of HCL Technologies Limited
		####################################################################

		apiVersion: v1
		kind: ConfigMap
		metadata:
		  name: <configmap_name>
		data:
		  plugins.properties: |
		      com.hcl.scheduling.agent.kubernetes
		      com.hcl.scheduling.agent.udeploycode
		      com.hcl.wa.plugin.ansible
		      com.hcl.wa.plugin.automationanywherebotrunner
		      com.hcl.wa.plugin.automationanywherebottrader
		      com.hcl.wa.plugin.awscloudformation
		      com.hcl.wa.plugin.awslambda
		      com.hcl.wa.plugin.awssns
		      com.hcl.wa.plugin.awssqs
		      com.hcl.wa.plugin.azureresourcemanager
		      com.hcl.wa.plugin.blueprism
		      com.hcl.wa.plugin.compression
		      com.hcl.wa.plugin.encryption
		      com.hcl.wa.plugin.gcpcloudstorage
		      com.hcl.wa.plugin.gcpdeploymentmanager
		      com.hcl.wa.plugin.jdedwards
		      com.hcl.wa.plugin.obiagent
		      com.hcl.wa.plugin.odiloadplan
		      com.hcl.wa.plugin.oraclehcmdataloader
		      com.hcl.wa.plugin.oracleucm
		      com.hcl.wa.plugin.saphanaxsengine
		      com.hcl.waPlugin.chefbootstrap
		      com.hcl.waPlugin.chefrunlist
		      com.hcl.waPlugin.obirunreport
		      com.hcl.waPlugin.odiscenario
		      com.ibm.scheduling.agent.apachespark
		      com.ibm.scheduling.agent.aws
		      com.ibm.scheduling.agent.azure
		      com.ibm.scheduling.agent.biginsights
		      com.ibm.scheduling.agent.centralizedagentupdate
		      com.ibm.scheduling.agent.cloudant
		      com.ibm.scheduling.agent.cognos
		      com.ibm.scheduling.agent.database
		      com.ibm.scheduling.agent.datastage
		      com.ibm.scheduling.agent.ejb
		      com.ibm.scheduling.agent.filetransfer
		      com.ibm.scheduling.agent.hadoopfs
		      com.ibm.scheduling.agent.hadoopmapreduce
		      com.ibm.scheduling.agent.j2ee
		      com.ibm.scheduling.agent.java
		      com.ibm.scheduling.agent.jobdurationpredictor
		      com.ibm.scheduling.agent.jobmanagement
		      com.ibm.scheduling.agent.jobstreamsubmission
		      com.ibm.scheduling.agent.jsr352javabatch
		      com.ibm.scheduling.agent.mqlight
		      com.ibm.scheduling.agent.mqtt
		      com.ibm.scheduling.agent.mssqljob
		      com.ibm.scheduling.agent.oozie
		      com.ibm.scheduling.agent.openwhisk
		      com.ibm.scheduling.agent.oracleebusiness
		      com.ibm.scheduling.agent.pichannel
		      com.ibm.scheduling.agent.powercenter
		      com.ibm.scheduling.agent.restful
		      com.ibm.scheduling.agent.salesforce
		      com.ibm.scheduling.agent.sapbusinessobjects
		      com.ibm.scheduling.agent.saphanalifecycle
		      com.ibm.scheduling.agent.softlayer
		      com.ibm.scheduling.agent.sterling
		      com.ibm.scheduling.agent.variabletable
		      com.ibm.scheduling.agent.webspheremq
		      com.ibm.scheduling.agent.ws

4) In the **plugins-config.yaml** file, assign a name of your choice to the configmap:

		name: <configmap_name>
		
5) Assign this same name to the `Global.customPlugins` parameter in the **values.yaml file**. See [Global parameters](#global-parameters) for more information about this global parameter.		

6) Delete the lines related to the integrations you do not want to make available in your environment. The remaining integrations will be integrated into Workload Automation at deployment time. Save your changes to the file.

   You can always refer back to this readme file and add an integration back into the file in the future. The integration becomes available the next time you update the agent containers.
   
7) To apply the configMap to your environment and integrate the plug-ins, run the following command:

        kubectl apply -f plugins_config.yaml -n <workload_automation_namespace>
	
	
Proceed to deploy the product components. After the deployment, you can include jobs related to these integrations when defining your workload.	

### Installing custom integrations
In addition to the integrations available on Automation Hub, you can extend Workload Automation with custom plug-ins that you create. For information about creating a custom plug-in, see [Workload Automation Lutist Development Kit](https://www.yourautomationhub.io/toolkit) on Automation Hub.

To install a custom plug-in and make it available to be used in your workload, perform the following steps before deploying or upgrading the agents:

1) Create a new folder with a name of your choosing, for example, "my_custom_plugins".

2) Create a Dockerfile with the following content and save it to the new folder as is, "my_custom_plugins". This file does not require any customization.

		FROM registry.access.redhat.com/ubi8:8.3

		ENV WA_BASE_UID=999
		ENV WA_BASE_GID=0
		ENV WA_USER=wauser
		ENV WA_USER_HOME=/home/${WA_USER}

		USER 0

		RUN echo "Creating \"${WA_USER}\" user for Workload Automation and assign it to group \"${WA_BASE_GID}\"" \
		&& userdel systemd-coredump \
		&& if  [ ${WA_BASE_GID} -ne 0 ];then \
		groupadd -g ${WA_BASE_GID} -r ${WA_USER};fi \
		&& /usr/sbin/useradd -u ${WA_BASE_UID} -m -d ${WA_USER_HOME} -r -g ${WA_BASE_GID} ${WA_USER}

		RUN mkdir -p /opt/wa_plugins /opt/wautils /tmp/custom_plugins
		COPY plugins/* /opt/wa_plugins/

		RUN chown -R ${WA_BASE_UID}:0 /opt/wa_plugins \
		&& chmod -R 755 /opt/wa_plugins

		COPY copy_custom_plugins.sh /opt/wautils/copy_custom_plugins.sh

		RUN chmod 755 /opt/wautils/copy_custom_plugins.sh \
		&& chown ${WA_BASE_UID}:${WA_BASE_GID} /opt/wautils/copy_custom_plugins.sh

		USER ${WA_BASE_UID}

		CMD [ "/opt/wautils/copy_custom_plugins.sh" ] 

3) Create another file specifically with the name: **copy_custom_plugins.sh**. The file must contain the following content, and it must be saved to the new folder, "my_custom_plugins":

		#!/bin/sh
		####################################################################
		# Licensed Materials Property of HCL*
		# (c) Copyright HCL Technologies Ltd. 2022. All rights reserved.
		#
		# * Trademark of HCL Technologies Limited
		####################################################################


		copyCustomPlugins(){
		    SOURCE_PLUGINS_DIR=$1
		    REMOTE_PLUGINS_DIR=$2


		    echo "I: Starting copy of custom plugins...." 
		    if [ -d "${SOURCE_PLUGINS_DIR}" ] && [ -d "${REMOTE_PLUGINS_DIR}" ];then
			echo "I: Copying custom plugins...." 
			cp --verbose -R ${SOURCE_PLUGINS_DIR} ${REMOTE_PLUGINS_DIR}
		    fi
		}

		###############
		#MAIN
		###############

		copyCustomPlugins $1 $2

4) Create a sub-folder specifically named: "plugins", in the new folder "my_custom_plugins".

5) Copy your custom .jar plug-ins to the "plugins" sub-folder.

6) Run the following command to build the Docker image:

        docker build -t <your_docker_registry_name>/<your_image_name>:<your_tag> .
	
   where <your_docker_registry_name> is the name of your docker registry, <your_image_name> is the name of your Docker image, and <your_tag> is the tag you assigned to your Docker image.
  
7) Run the following command to push the Docker image to the registry:

        docker push <your_docker_registry_name>/<your_image_name>:<your_tag>
	
8) Configure the `customPluginImageName` parameter in the values.yaml file with the name of the image and tag built in the previous steps. See [Global parameters](#global-parameters) for more information about this parameter.

        customPluginImageName: <your_docker_registry_name>/<your_image_name>:<your_tag>
	
Proceed to deploy the product components. After the deployment, you can include jobs related to your custom plug-ins when defining your workload.	

## Upgrading the Chart

Before you upgrade a chart, verify if there are jobs currently running and manually stop the related processes or wait until the jobs complete.	To upgrade the release <workload_automation_release_name> to a new version of the chart, run the following command from the directory where the values.yaml file is located:

 `helm upgrade release_name <repo_name>/workload-automation-prod -f values.yaml -n <workload_automation_namespace>`

If you have configured a configMaps file as described in [Installing Automation Hub integrations](installing-automation-hub-integrations), this upgrade procedure automatically upgrades any integrations or plug-ins previously installed from Automation Hub.

## Rolling Back the Chart

Before you roll back a chart, verify if there are jobs currently running and manually stop the related processes or wait until the jobs complete. To roll back the <workload_automation_release_name> release to a previous version of the chart, run the following commands:

1.	Identify the revision number to which you want to roll back by running the command:

        helm history <workload_automation_release_name> -n <workload_automation_namespace>
		
2.	Roll back to the specified revision number:

        helm rollback <workload_automation_release_name>  <revision-number> -n <workload_automation_namespace>
  
## Uninstalling the Chart

 To uninstall the deployed components associated with the chart and clean up the orphaned Persistent Volumes, run:
 
 1. Uninstall the workload-automation-prod deployment, run:

        helm uninstall release_name -n <workload_automation_namespace> 

    The command removes all of the Kubernetes components associated with the chart and uninstalls the <workload_automation_release_name> release.
	
2. Clean up orphaned Persistent Volumes by running the following command:

       kubectl delete pvc -l <workload_automation_release_name> -n <workload_automation_namespace> 

		
## Configuration Parameters

The following tables list the configurable parameters of the chart, **values.yaml**, an example of the values and the default values. The tables are organized as follows:

- **[Global parameters](#global-parameters)** (all product components)
- **[Agent parameters](#agent-parameters)**
- **[Dynamic Workload Console parameters](#dynamic-workload-console-parameters)**
- **[Server parameters](#server-parameters)** (master domain manager)  

&nbsp;  
-  #### Global parameters
The following table lists the global configurable parameters of the chart relative to all product components and an example of their values:

| **Parameter**                           | **Description**                                                                                                                                                                                                                                                              | **Mandatory**  | **Example**                      | **Default**                      |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------  | -------------------------------- | -------------------------------- |
| global.license                          | Use ACCEPT to agree to the license agreement                                                                                                                                                                                                                                 | yes            | not accepted                     | not accepted                     |
| global.enableServer                     | If enabled, the Server application is deployed                                                                                                                                                                                                                               | no             | true                             | true                             |
| global.enableConsole                    | If enabled, the Console application is deployed                                                                                                                                                                                                                              | no             | true                             | true                             |
| global.enableAgent                      | If enabled, the Agent application is deployed                                                                                                                                                                                                                                | no             | true                             | true                             |
| global.serviceAccountName               | The name of the serviceAccount to use. The IBM Workload Automation default service account (**wauser**) and not the default cluster account                                                                                                                                                                                                                                          | no             | default                          | ** wauser**            |
| global.language                         | The language of the container internal system. The supported language are: en (English), de (German), es (Spanish), fr (French), it (Italian), ja (Japanese), ko (Korean), pt_BR (Portuguese (BR)), ru (Russian), zh_CN (Simplified Chinese) and zh_TW (Traditional Chinese) | yes            | en                               | en                               |  
| global.customLabels                      | This parameter contains two fields: *name* and *value*. Insert customizable labels to group resources linked together.                                                                                                                                                                                                                               | no             | name: environment value: prod                             | name: environment value: prod                             |
| global.enablePrometheus                          | Use to enable (true) or disable (false) Prometheus metrics                                                                                                                                                                                                                                 | no            | true                     |true                     |
| global.customPlugins                          | If specified, the plug-ins and integrations listed in the configMap file are automatically installed when deploying the server and console containers.  See [Installing Automation Hub integrations](#installing-automation-hub-integrations) for details about the procedure.                                                                                                                                                                                                                            | no            | mycustomplugin (the value specified must match the value specified in the configMap file)   |        |
| global.customPluginsImageName                 | To install a custom plug-in when deploying the server and console containers, specify the name of the Docker registry, the plug-in image, and the tag assigned to the Docker image. See [Installing custom integrations](#installing-custom-integrations) for details about the procedure.                                                                                                                                                                                                                             | no            | myregistry/mypluginimage:my_tag   |        |

-  #### Agent parameters
The following table lists the configurable parameters of the chart relative to the agent and an example of their values:

| **Parameter**                                    | **Description**                                                                                                                                                                                                                                                      | **Mandatory**  | **Example**                      | **Default**                      |
| ------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | -------------  | -------------------------------- | -------------------------------- |
| waagent.fsGroupId                                | The secondary group ID of the user                                                                                                                                                                                                                                   | no             | 999                              |                                  |
| waagent.supplementalGroupId                      | Supplemental group id of the user                                                                                                                                                                                                                                    | no             |                                  |                                  |
| waagent.replicaCount                             | Number of replicas to deploy                                                                                                                                                                                                                                         | yes            | 1                                | 1                                |
| waagent.image.repository                         | IBM Workload Automation Agent image repository                                                                                                                                                                                                                                | yes            | cp.icr.io/cp/ibm-workload-automation-agent        | cp.icr.io/cp/ibm-workload-automation-agent        |
| waagent.image.tag                                | IBM Workload Automation Agent image tag                                                                                                                                                                                                                                       | yes            | 10.1.0.01.20221130                        | 10.1.0.01.20221130                        |
| waagent.image.pullPolicy                         | image pull policy                                                                                                                                                                                                                                                    | yes            | Always                           | Always                           |
| waagent.licenseType                            | Product license type (IBM Workload Scheduler only)                                                                                                                                                                                                                                                             | yes           | PVU                           | PVU                       |
| waagent.agent.name                               | Agent display name                                                                                                                                                                                                                                                   | yes            | WA_AGT                           | WA_AGT                           |
| waagent.agent.tz                                 | If used, it sets the TZ operating system environment variable                                                                                                                                                                                                        | no             | America/Chicago                  |                                  |
| waagent.agent.networkpolicyEgress                                 | Customize egress policy. Controls network traffic and how a component pod is allowed to communicate with other pods. If empty, no egress policy is defined                                                                                                                                                                                                        | no             | See [Network enablement](#network-enablement)                  |                                  |
| waagent.agent.nodeAffinityRequired                 | A set of rules that determines on which nodes an agent can be deployed using custom labels on nodes and label selectors specified in pods.                                                                                                                                                                                                                           | no             |   See [Network enablement](#network-enablement)              |                               |
| waagent.agent.dynamic.server.mdmhostname         | Hostname or IP address of the master domain manager                                                                                                                                                                                                                  | no (mandatory if a server is not present inside the same namespace)            | wamdm.demo.com                   |                                  |
| waagent.agent.dynamic.server.port                | The HTTPS port that the dynamic agent must use to connect to the master domain manager                                                                                                                                                                               | no             | 31116                            | 31116                            |
| waagent.agent.dynamic.pools*                      | The static pools of which the Agent should be a member                                                                                                                                                                                                               | no             | Pool1, Pool2                     |                                  |
| waagent.agent.dynamic.useCustomizedCert          | If true, customized SSL certificates are used to connect to the master domain manager                                                                                                                                                                                | no             | false                            | false                            |
| waagent.agent.dynamic.certSecretName             | The name of the secret to store customized SSL certificates                                                                                                                                                                                                          | no             | waagent-cert-secret              |                                  |
| waagent.agent.containerDebug                     | The container is executed in debug mode                                                                                                                                                                                                                              | no             | no                               | no                               |
| waagent.agent.livenessProbe.initialDelaySeconds  | The number of seconds after which the liveness probe starts checking if the server is running                                                                                                                                                                        | yes            | 60                               | 60                               | 
| waagent.resources.requests.cpu                   | The minimum CPU requested to run                                                                                                                                                                                                                                     | yes            | 200m                             | 200m                             | 
| waagent.resources.requests.memory                | The minimum memory requested to run                                                                                                                                                                                                                                  | yes            | 200Mi                            | 200Mi                            | 
| waagent.resources.limits.cpu                     | The maximum CPU requested to run                                                                                                                                                                                                                                     | yes            | 1                                | 1                                | 
| waagent.resources.limits.memory                  | The maximum memory requested to run                                                                                                                                                                                                                                  | yes            | 2Gi                              | 2Gi                              | 
| waagent.persistence.enabled                      | If true, persistent volumes for the pods are used                                                                                                                                                                                                                    | no             | true                             | true                             |
| waagent.persistence.useDynamicProvisioning       | If true, StorageClasses are used to dynamically create persistent volumes for the pods                                                                                                                                                                               | no             | true                             | true                             | 
| waagent.persistence.dataPVC.name                 | The prefix for the Persistent Volumes Claim name                                                                                                                                                                                                                     | no             | data                             | data                             |
| waagent.persistence.dataPVC.storageClassName     | The name of the Storage Class to be used. Leave empty to not use a storage class                                                                                                                                                                                     | no             | nfs-dynamic                      |                                  |
| waagent.persistence.dataPVC.selector.label       | Volume label to bind (only limited to single label)                                                                                                                                                                                                                  | no             | my-volume-label                  |                                  |
| waagent.persistence.dataPVC.selector.value       | Volume label value to bind (only limited to single value)                                                                                                                                                                                                            | no             | my-volume-value                  |                                  |
| waagent.persistence.dataPVC.size                 | The minimum size of the Persistent Volume                                                                                                                                                                                                                            | no             | 2Gi                              | 2Gi                              |
| waserver.persistence.extraVolumes               | A list of additional extra volumes                                                                                                                                                                                                                             | no            | custom-volume-1                             |                                              |
| waserver.persistence.extraVolumeMounts               | A list of additional extra volumes mounts                                                                                                                                                                                                                            | no            | custom-volume-1                             |                                              |


## Configuring

The following procedures are ways in which you can configure the default deployment of the product components. They include the following configuration topics:

* [Network enablement](#network-enablement)
* [Enabling communication between product components in an on-premises offering with components in the Cloud](#enabling-communication-between-product-components-in-an-on-premises-offering-with-components-in-the-cloud)
* [Scaling the product](#scaling-the-product)
* [Managing your custom certificates](#managing-your-custom-certificates)

### Network enablement

The IBM Workload Automation agents can use two different ways to route external traffic into the Kubernetes Service cluster:

* A **load balancer** service that redirects traffic
* An **ingress** service that manages external access to the services in the cluster

You can freely switch between these two types of configuration.

#### Network policy

You can specify an egress network policy to include a list of allowed egress rules for the agents. Each rule allows traffic leaving the cluster which matches both the "to" and "ports" sections. For example, the following sample demonstrates how to allow egress to another destination:

networkpolicyEgress:

	- name: to-mdm
	  egress:
	  - to:
	    - podSelector:
	        matchLabels:
		  app.kubernetes.io/name: waagent
	    - port: <agent_port>
	      protocol: TCP
	- name: dns
	  egress:
	    - to:
	      - namespaceSelector:
	          matchLabels:
		    name: kube-system
	    - ports:
	        - port: 53
		  protocol: UDP
		- port: 53
		  protocol: TCP

For more information, see [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/).

#### Node affinity Required
You can also specify node affinity required to determine on which nodes a component can be deployed using custom labels on nodes and label selectors specified in pods. The following is an example:

nodeAffinityRequired:

	-key: iwa-node
	  operator: In
	  values:
	  - 'true'

where iwa-node represents the value of the node affinity required.

#### Load balancer service  

- **Agent:**

  To configure a load balancer for the server, follow these steps:

1. Locate the following parameters in the `values.yaml` file:

		exposeServiceType
		exposeServiceAnnotation

For more information about these configurable parameters, see the **[Server parameters](#server-parameters)** table.

2. Set the value of the `exposeServiceType` parameter to `LoadBalancer`.

3. In the `exposeServiceAnnotation` section, uncomment the lines in this section as follows:

![Amazon EKS](images/tagawseks.png "Amazon EKS") 

		service.beta.kubernetes.io/aws-load-balancer-type: nlb
		service.beta.kubernetes.io/aws-load-balancer-internal: "true"
		
![Microsoft Azure](images/tagmsa.png "Microsoft Azure") 

		service.beta.kubernetes.io/azure-load-balancer-internal: "true"

![Google GKE](images/taggke.png "Google GKE") 

		networking.gke.io/load-balancer-type: "Internal"


4. Specify the load balancer type and set the load balancer to internal by specifying "true".


#### Ingress service 

- **Agent:**

  To configure an ingress for the agent, follow these steps:

1. Locate the following parameters in the `values.yaml` file:

		exposeServiceType
		exposeServiceAnnotation

   For more information about these configurable parameters, see the **[Server parameters](#server-parameters)** table.

2. Set the value of the `exposeServiceType`parameter to `Ingress`.

3. In the `exposeServiceAnnotation` section, leave the following lines as comments:

![Amazon EKS](images/tagawseks.png "Amazon EKS") 

		#service.beta.kubernetes.io/aws-load-balancer-type:nlb
		#service.beta.kubernetes.io/aws-load-balancer-internal: "true"

![Microsoft Azure](images/tagmsa.png "Microsoft Azure")

		#service.beta.kubernetes.io/azure-load-balancer-internal: "true"	
		
![Microsoft Azure](images/taggke.png "Microsoft Azure")

                #networking.gke.io/load-balancer-type: "Internal"



#### Ingress controller

To manage the ingress service for versions earlier than 10.1 Fix Pack 1, ensure an ingress controller is correctly configured. For example, to configure an NGINX ingress controller, 
ensure the following option is set if NGINX is installed using a Helm chart: `"controller.extraArgs.enable-ssl-passthrough"`. 
Refer to the [NGINX Ingress Controller documentation]
(https://kubernetes.github.io/ingress-nginx/) for more details.	

		

### Scaling the product 

By default a single agent is installed. **Only for dynamic agents:** If you want to change the topology for IBM Workload Automation, increase or decrease the values of the `replicaCount` parameter in the `values.yaml` file for each agent and save the changes.

#### Scaling up or down (only for dynamic agents)

To scale one or more IBM Workload Automation agents up or down:

 Modify the values of the `replicaCount` parameter in the `values.yaml` file for each agent accordingly, and save the changes.
 

>**Note**:
> -   When you scale down each type of component, the persistent volume (PV) that the storage class created for the pod instance is not deleted to avoid losing data should the scale down not be desired. When you need to perform a subsequent scaling up, new component instances are installed by using the old PVs.
>-   When you scale down agent component, the workstation definitions are not removed from the database, so you can manually delete them or set them to ignore to avoid having a non-working workstation in the plan. 

#### Scaling to 0
The IBM Workload Automation Helm chart does not support automatic scaling to zero. If you want to manually scale the dynamic agent to zero, set the value of the `replicaCount` parameter to zero. To maintain the current IBM Workload Automation scheduling and topology, do not set the `replicaCount` value for the agent components to zero.

#### Proportional scaling
 The IBM Workload Automation Helm chart does not support proportional scaling.
		  
### Managing your custom certificates

If you use customized certificates, `useCustomizedCert:true`, you must create a secret containing the customized files that will replace the agent default ones in the \<workload_automation_namespace>. Customized files must have the same name as the default ones.

  * TWSClientKeyStoreJKS.sth
  * TWSClientKeyStore.kdb
  * TWSClientKeyStore.sth
  * TWSClientKeyStoreJKS.jks
  * TWSServerTrustFile.jks
  * TWSServerTrustFile.jks.pwd
  * TWSServerKeyFile.jks
  * TWSServerKeyFile.jks.pwd
  * ltpa.keys  (The ltpa.keys certificate is required only if you use Single Sign-On with LTPA)
    
  If you want to use custom certificates, set `useCustomizedCert:true` and use kubectl to apply the secret in the \<workload_automation_namespace>.
 
 For the z-centric and dynamic agents, type the following command:
 ```
 kubectl create secret generic waagent-cert-secret --from-file=TWSClientKeyStore.kdb --from-file=TWSClientKeyStore.sth --from-file=TWSClientKeyStoreJKS.jks --from-file=TWSClientKeyStoreJKS.sth -n <workload_automation_namespace>    
 ```   
    
   where, TWSClientKeyStoreJKS.sth, TWSClientKeyStore.kdb, TWSClientKeyStore.sth, TWSClientKeyStoreJKS.jks, TWSServerTrustFile.jks and TWSServerKeyFile.jks are the Container keystore and stash file containing your customized certificates.
   
   For details about custom certificates, see [Connection security overview](https://www.ibm.com/docs/workload-scheduler/10.1.0?topic=configuration-connection-security-overview).
    

> **Note**: Passwords for "TWSServerTrustFile.jks" and "TWSServerKeyFile.jks" files must be entered in the respective "TWSServerTrustFile.jks.pwd" and "TWSServerKeyFile.jks.pwd" files.
 
        
If you define custom certificates, you are in charge of keeping them up to date, therefore, ensure you check their duration and plan to rotate them as necessary. To rotate custom certificates, delete the previous secret and upload a new secret, containing new certificates. The pod restarts automatically and the new certificates are applied.


## Storage

### Storage requirements for the workload

IBM Workload Automation requires persistent storage for each agent that you deploy, to maintain the scheduling workload and topology.


To make all of the configuration and runtime data persistent, the Persistent Volume you specify must be mounted in the following container folder:

`/home/wauser`

The Pod is based on a StatefulSet. This guarantees that each Persistent Volume is mounted in the same Pod when it is scaled up or down.

For test purposes only, you can configure the chart so that persistence is not used.

IBM Workload Automation can use either dynamic provisioning or static provisioning using a pre-created persistent volume  
to allocate storage for each component that you deploy. You can pre-create Persistent Volumes to be bound to the StatefulSet using Label or StorageClass. It is highly recommended to use persistence with dynamic provisioning. In this case, you must have defined your own Dynamic Persistence Provider. IBM Workload Automation supports the following provisioning use cases:

* Kubernetes dynamic volume provisioning to create both a persistent volume and a persistent volume claim.
This type of storage uses the default storageClass defined by the Kubernetes admin or by using a custom storageClass which overrides the default. Set the values as follows:

	* **persistence.enabled:true (default)**
	* **persistence.useDynamicProvisioning:true(default)**

Specify a custom storageClassName per volume or leave the value blank to use the default storageClass.

* Persistent storage using a predefined PersistentVolume set up prior to the deployment of this chart. 
Pre-create a persistent volume. If you configure the label=value pair described in the following **Note**, then the persistent volume claim is automatically generated by the Helm chart and bound to the persistent volume you pre-created. Set the global values as follows:

	* **persistence.enabled:true**
	* **persistence.useDynamicProvisioning:false**

> Note: By configuring the following two parameters, the persistent volume claim is automatically generated. Ensure that this label=value pair is inserted in the persistent volume you created: 

- \<wa-component>.persistence.dataPVC.selector.label
- \<wa-component>.persistence.dataPVC.selector.value

Let the Kubernetes binding process select a pre-existing volume based on the accessMode and size. Use selector labels to refine the binding process.

Before you deploy all of the components, you have the opportunity to choose your persistent storage from the available persistent storage options in AWS Elastic Kubernetes Service that are supported by IBM Workload Automation or, you can leave the default storageClass.  
For more information about all of the supported storage classes, see the table in [Storage classes static PV and dynamic provisioning](#storage-classes-static-pv-and-dynamic-provisioning).

If you create a storageClass object or use the default one, ensure that you have a sufficient amount of backing storage for your IBM Workload Automation components.  
For more information about the required amount of storage you need for each component, see the [Resources Required](#resources-required) section.

_Custom storage class:_  
Modify the the `persistence.dataPVC.storageClassName` parameter in the YAML file by specifying the custom storage class name, when you deploy the IBM Workload Automation product components.

_Default storage class:_  
Leave the values for the persistence.dataPVC.storageClassName parameter blank in the YAML file when you deploy the IBM Workload Automation product components.  
For more information about the storage parameter values to set in the YAML file, see the tables, [Agent parameters](#agent-parameters),  [Dynamic Workload Console parameters](#dynamic-workload-console-parameters), and [Server parameters](#server-parameters) (master domain manager).


### File system permissions

File system security permissions need to be well known to ensure uid, gid, and supplemental gid requirements can be satisfied.
On Kubernetes native, UID 999 is used.

### Persistent volume storage access modes

IBM Workload Automation supports only ReadWriteOnce (RWO) access mode. The volume can be mounted as read-write by a single node.

## Metrics monitoring 

IBM Workload Automation uses Grafana to display performance data related to the product. This data includes metrics related to the server and console application servers (WebSphere Application Server Liberty Base), your workload, your workstations, critical jobs, message queues, the database connection status, and more. Grafana is an open source tool for visualizing application metrics. Metrics provide insight into the state, health, and performance of your deployments and infrastructure. IBM Workload Automation cloud metric monitoring uses an opensource Cloud Native Computing Foundation (CNCF) project called Prometheus. It is particularly useful for collecting time series data that can be easily queried. Prometheus integrates with Grafana to visualize the metrics collected.



The following metrics are collected and available to be visualized in the preconfigured Grafana dashboard. The dashboard is named **<workload_automation_namespace> <workload_automation_release_name>**:

| Metric Display Name    | Metric Name                         | Description                                                                                                                               |
|-----------------------------------------|--------------------------|-----------------------------------------------------------------------------------------------------------------|

| Workload                                | application_wa_JobsInPlanCount_jobs   |Workload by job status: WAITING, READY, HELD, BLOCKED, CANCELED, ERROR, RUNNING, SUCCESSFUL, SUPPRESS, UNDECIDED  |
|                               | application_wa_JobsByWorkstation    |Job status by workstation  |
|                               | application_wa_JobsByFolder_jobs    |Job status by folder  |
|                               | application_wa_JobsInPlanCount_jobs  | Workload throughput (jobs/minute)
| Critical Jobs              | application_wa_criticalJob_incompletePredecessor    |Incomplete predecessors  |
|                            | application_wa_criticalJob_potentialRisk_boolean   |Risk level:  potential risk  |
|                            | application_wa_criticalJob_highRisk_boolean |Risk level: high risk  |
|                            | application_wa_criticalJob_estimateEnd_seconds  | Estimated end  |
|                            | application_wa_criticalJob_confidence_factor     | Confidence factor  |
| WA Server - Internal Message Queues | application_wa_msgFileFill_percent    | Internal message queue usage for Appserverbox.msg, Courier.msg, mirrorbox.msg, Mailbox.msg, Monbox.msgn, Moncmd.msg, auditbox.msg, clbox.msg, planbox.msg, Intercom.msg, pobox messages, and server.msg   |
| Workstation Status     | application_wa_workstation_running | Workstations running  |
|                        | application_wa_workstation_linked_boolean | Workstations linked  |
| Database Connection Status  | application_wa_DB_connected_boolean            |  1 - connected, 0 - not connected   |
| PODs        |  kube_pod_container_status_restarts_total | Pod restarts (server, console, and agent) |
|             |  kube_pod_status_phase                     | Failing pods (server, console, and agent)|
|             |  container_cpu_usage_seconds_total          | POD CPU usage (server, console, and agent)        |
|             |  container_network_transmit_bytes_total  |  Network I/O (server and console)        |
|             |  container_network_receive_bytes_total  |  Network I/O (server and console)        |
|             |  container_memory_usage_bytes          | RAM usage (server, console, and agent)       |
| Persistent Volumes | kubelet_volume_stats_used_bytes   | For server, console, and agent: volume capacity (used, free)      |
| WA  Server and Console - Liberty  |  base_memory_usedHeap_bytes     | Heap usage percentage  |
|                                   | vendor_session_activeSessions   | Active sessions  |
|                                   | vendor_session_liveSessions     |   Live sessions  |
|                                   | vendor_threadpool_activeThreads | Active threads  |
|                                   | vendor_threadpool_size          | Threadpool size  |
|                                   | base_gc_time_seconds            |Time per garbage collection cycle moving average  |
| WA Sever and Console - Connection Pools (Liberty) | vendor_connectionpool_inUseTime_total_seconds | Average time usage per connection over last |
|                                   | vendor_connectionpool_managedConnections| Managed connections |
|                                   | vendor_connectionpool_freeConnections | Free connections |
|                                   | vendor_connectionpool_connectionHandles  | Connection handles |
|                                   | vendor_connectionpool_destroy_total  | Created and destroyed connections |

The following is an example of the various metrics available with focus on the workload job status:

![Job status](images/WA_dashboard_jobstats.png)
 

 The following is an exammple of how persistent volume capacity for the server, console and agent are visualized:
 
  ![Job status](images/WA_dashboard_pv.png)
  
  ### Setting the Grafana service
  Before you set the Grafana service, ensure that you have already installed Grafana and Prometheus on your cluster. For information about deploying Grafana see [Install Grafana](https://github.com/helm/charts/blob/master/stable/grafana/README.md). For information about deploying the open-source Prometheus project see [Download Promotheus](https://github.com/helm/charts/tree/master/stable/prometheus).
  
1. Log in to your cluster. To identify where Grafana is deployed, retrieve the value for the \<grafana-namespace> by running:
  
          helm list -A
		  
2. Download the grafana_values.yaml file by running:

        helm get values grafana -o yaml -n <grafana-namespace> grafana_values.yaml

3. Modify the grafana_values.yaml file by setting the following parameter values:	

	    dashboards:
		     SCProvider: true
		     enabled: true
		     folder: /tmp/dashboards
		     label: grafana_dashboard
		     provider:
			 allowUiUpdates: false
			 disableDelete: false
			 folder: ""
			 name: sidecarProvider
			 orgid: 1
			 type: file
		     searchNamespace: ALL

4. Update the grafana_values.yaml file in the Grafana pod by running the following command:

    ` helm upgrade grafana stable/grafana -f grafana_values.yaml -n <grafana-namespace>`
					 
5. To access the Grafana console:

     a. Get the EXTERNAL-IP address value of the Grafana service by running:
	 
        kubectl get services -n <grafana-namespace>
		
     b. Browse to the EXTERNAL-IP address and log in to the Grafana console.		
 
### Viewing the preconfigured dashboard in Grafana

To get an overview of the cluster health, you can view a selection of metrics on the predefined dashboard:

1. In the left navigation toolbar, click **Dashboards**.

2. On the **Manage** page, select the predefined dashboard named **<workload_automation_namespace> <workload_automation_release_name>**.

For more information about using Grafana dashboards see [Dashboards overview](https://grafana.com/docs/grafana/latest/features/dashboard/dashboards/).


## Limitations

*  Limited to amd64 platforms.
*  Anonymous connections are not permitted.
*  When sharing Dynamic Workload Console resources, such as tasks, engines, scheduling objects and so on, to groups, ensure the user sharing the resource is a member of the group to which the user is sharing the resourc.
*  LDAP configuration on the chart is not supported. Manual configuration is required using the traditional LDAP configuration. 


## Documentation

To access the complete product documentation library for IBM Workload Automation, see https://www.ibm.com/docs/en/workload-automation/10.1.0?topic=z-workload-scheduler .


## Troubleshooting


In the event a problem should occur while using IBM Workload Automation, Customer Support might ask you to supply information about your system and environment to perform problem determination. The following utilities are available:

-   A general data capture utility command that extracts information about IBM Workload Automation  and related agent workstations, system-specific information, and data related to WebSphere Application Server Liberty Base; see [Data capture utility](https://www.ibm.com/docs/workload-scheduler/10.1.0?topic=data-capture-utility).

-   A first failure data capture (ffdc) facility built into **batchman** and **mailman** that automatically runs the data capture utility when failures occur in **jobman**, **mailman**, or **batchman** and collects products logs, traces and configuration files; see [First failure data capture (ffdc)](https://www.ibm.com/docs/workload-scheduler/10.1.0?topic=data-first-failure-capture-ffdc).

-   The data capture utility script, wa_pull_info, is also used to collect data related to the Dynamic Workload Console to assist in problem determination; see [Data capture utility](https://www.ibm.com/docs/workload-scheduler/10.1.0?topic=data-capture-utility).

-   WebSphere Application Server Liberty Base javadump command to create the heap dump for WebSphere Application Server Liberty Base that runs on the Dynamic Workload Console and the master domain manager; see [Creating application server dumps](https://www.ibm.com/docs/workload-scheduler/10.1.0?topic=data-creating-application-server-dumps).


In case of problems related to deploying the product with containers, see [Troubleshooting](https://www.ibm.com/docs/workload-scheduler/10.1.0?topic=containers-troubleshooting).
